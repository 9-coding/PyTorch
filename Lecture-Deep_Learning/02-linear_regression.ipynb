{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjWrCBvWxZMoir6qhNiDoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/PyTorch/blob/main/Lecture-Deep_Learning/02-linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5nnkWsnSOj6",
        "outputId": "683ec38c-fe2a-40ad-c153-1c1117d61442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1]) torch.Size([8, 1])\n",
            "< Before Train >\n",
            "W =  tensor([0.], requires_grad=True)\n",
            "b = tensor([0.], requires_grad=True)\n",
            "Epoch: 0 , Cost: 4653.625000\n",
            "Epoch: 10 , Cost: 94.874390\n",
            "Epoch: 20 , Cost: 94.867035\n",
            "Epoch: 30 , Cost: 94.859619\n",
            "Epoch: 40 , Cost: 94.852295\n",
            "Epoch: 50 , Cost: 94.844971\n",
            "Epoch: 60 , Cost: 94.837578\n",
            "Epoch: 70 , Cost: 94.830231\n",
            "Epoch: 80 , Cost: 94.822861\n",
            "Epoch: 90 , Cost: 94.815521\n",
            "Epoch: 100 , Cost: 94.808197\n",
            "Epoch: 110 , Cost: 94.800865\n",
            "Epoch: 120 , Cost: 94.793488\n",
            "Epoch: 130 , Cost: 94.786156\n",
            "Epoch: 140 , Cost: 94.778816\n",
            "Epoch: 150 , Cost: 94.771484\n",
            "Epoch: 160 , Cost: 94.764137\n",
            "Epoch: 170 , Cost: 94.756813\n",
            "Epoch: 180 , Cost: 94.749512\n",
            "Epoch: 190 , Cost: 94.742142\n",
            "Epoch: 200 , Cost: 94.734818\n",
            "Epoch: 210 , Cost: 94.727509\n",
            "Epoch: 220 , Cost: 94.720161\n",
            "Epoch: 230 , Cost: 94.712868\n",
            "Epoch: 240 , Cost: 94.705528\n",
            "Epoch: 250 , Cost: 94.698273\n",
            "Epoch: 260 , Cost: 94.690941\n",
            "Epoch: 270 , Cost: 94.683578\n",
            "Epoch: 280 , Cost: 94.676346\n",
            "Epoch: 290 , Cost: 94.668999\n",
            "Epoch: 300 , Cost: 94.661713\n",
            "Epoch: 310 , Cost: 94.654427\n",
            "Epoch: 320 , Cost: 94.647125\n",
            "Epoch: 330 , Cost: 94.639801\n",
            "Epoch: 340 , Cost: 94.632515\n",
            "Epoch: 350 , Cost: 94.625259\n",
            "Epoch: 360 , Cost: 94.617973\n",
            "Epoch: 370 , Cost: 94.610641\n",
            "Epoch: 380 , Cost: 94.603378\n",
            "Epoch: 390 , Cost: 94.596123\n",
            "Epoch: 400 , Cost: 94.588837\n",
            "Epoch: 410 , Cost: 94.581573\n",
            "Epoch: 420 , Cost: 94.574280\n",
            "Epoch: 430 , Cost: 94.567039\n",
            "Epoch: 440 , Cost: 94.559731\n",
            "Epoch: 450 , Cost: 94.552475\n",
            "Epoch: 460 , Cost: 94.545166\n",
            "Epoch: 470 , Cost: 94.537979\n",
            "Epoch: 480 , Cost: 94.530708\n",
            "Epoch: 490 , Cost: 94.523468\n",
            "< After Train >\n",
            "W = tensor([0.9664], requires_grad=True)\n",
            "b = tensor([0.1485], requires_grad=True)\n",
            "My final predict socre is estimated as 68.765038\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Define parameters: W - weight , b - bias\n",
        "# requires_grad: backpropagation을 허용할 것인가.\n",
        "W = torch.zeros(1, requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "x_train = torch.FloatTensor([[78],[83],[56],[67],[85],[44],[32],[90]])\n",
        "y_train = torch.FloatTensor([[66],[73],[76],[65],[81],[54],[29],[85]])\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr = 0.0001)\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "print('< Before Train >')\n",
        "print('W = ', W)\n",
        "print('b =', b)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  hypothesis = x_train * W + b\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print('Epoch: %d , Cost: %f' %(epoch, cost))\n",
        "\n",
        "print('< After Train >')\n",
        "print('W =', W)\n",
        "print('b =', b)\n",
        "\n",
        "new_input = 71\n",
        "predict = W * new_input + b # H(x) = 0.9664x + 0.1485\n",
        "print('My final predict socre is estimated as %f' %(predict))"
      ]
    }
  ]
}