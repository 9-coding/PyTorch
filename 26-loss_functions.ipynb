{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0TcfSW8zCPSGKXvuwTORB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/PyTorch/blob/main/26-loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNkJ9xK0SZq4"
      },
      "source": [
        "# Loss Functions\n",
        "인공지능 모델이 data와 얼마나 잘 맞는지 측정하는 function.\n",
        "- loss 값은 function에 따라 달라질 수 있음.\n",
        "- 인공지능 학습은 loss를 최소화하는 방향으로 진행.\n",
        "\n",
        "Binary Classification\n",
        "- BCELoss or BCEWithLogitsLoss\n",
        "\n",
        "Multiple-class Classification:\n",
        "- CrossEntropyLoss\n",
        "- LogSoftmax + NLLLoss\n",
        "\n",
        "Multi-label Classification:\n",
        "- sigmoid + BCELoss\n",
        "- MultiLablelSoftMarginLoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "UL_2NAbcGEIT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification"
      ],
      "metadata": {
        "id": "KuQ83D48JrCR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYlkZr1SfMF"
      },
      "source": [
        "## BCELoss\n",
        "binary classification에 사용.\n",
        "- model의 출력이 0과 1 사이 값이어야 하기 때문에 출력으로 sigmoid 사용.\n",
        "- pred: sigmoid\n",
        "- label: 0과 1 사이의 실제 라벨값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35ut6qv2Se40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a723619f-fa9c-4236-9d14-3c5d7ab93f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCELoss: 0.514159619808197\n"
          ]
        }
      ],
      "source": [
        "# 예측값과 실제값\n",
        "outputs = torch.tensor([0.7, 0.2, 0.9], requires_grad=True).float()\n",
        "targets = torch.tensor([1.0, 0.0, 1.0]).float()\n",
        "\n",
        "# BCELoss 선언\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 시그모이드 활성화 적용\n",
        "outputs = torch.sigmoid(outputs)\n",
        "\n",
        "# 손실 계산\n",
        "loss = criterion(outputs, targets)\n",
        "print('BCELoss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0321B4TSpAS"
      },
      "source": [
        "## BCEWithLogitsLoss\n",
        "- BCELoss의 variation.\n",
        "- 내부적으로 sigmoid를 결합시켰으므로 입력으로 logit score를 받을 수 있음.\n",
        "- pred: sigmoid의 입력값인 **logit score**\n",
        "- label: 0과 1 사이의 실제 라벨 값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ra2a870SZdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7936d8dc-f9bb-48f1-dc25-2e483022d2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCEWithLogitsLoss: 0.3048\n"
          ]
        }
      ],
      "source": [
        "# 원시 출력값 (시그모이드 활성화 적용 전 raw score)\n",
        "raw_outputs = torch.tensor([0.5, -1.0, 2.0], requires_grad=True)\n",
        "targets = torch.tensor([1.0, 0.0, 1.0])\n",
        "\n",
        "# BCEWithLogitsLoss 선언\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 손실 계산 (시그모이드 내부 적용)\n",
        "loss = criterion(raw_outputs, targets)\n",
        "print(f'BCEWithLogitsLoss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class classification"
      ],
      "metadata": {
        "id": "5DVFVHylJ7jj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaI-waGVSsiF"
      },
      "source": [
        "## CrossEntropyLoss\n",
        "내부적으로 softmax 함수 적용.\n",
        "- input: logit score vector\n",
        "- output: class index. `dtype=long(int64)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OsY8asbLSoOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02911d8c-1e65-4cab-ac3b-cb3f247e3fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n",
            "CrossEntropyLoss: 0.74\n"
          ]
        }
      ],
      "source": [
        "# 원시 출력값 (softmax 활성화 적용 전)\n",
        "raw_outputs = torch.tensor(\n",
        "\t[[1.0, 2.0, 3.0],\n",
        "         [1.0, 2.0, 0.0],\n",
        "         [0.0, 2.0, 1.0]],\n",
        "        requires_grad=True).float()\n",
        "targets = torch.tensor([2, 0, 1]).long()  # 각 샘플의 클래스 인덱스\n",
        "print(targets.dtype)\n",
        "# CrossEntropyLoss 선언\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 손실 계산 (softmax 내부 적용)\n",
        "loss = criterion(raw_outputs, targets)\n",
        "print(f'CrossEntropyLoss: {loss.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drv_YTj-S35R"
      },
      "source": [
        "## NLLLoss\n",
        "Negative Log Likelihood Loss\n",
        "- 주로 LogSoftmax와 함께 사용됨.\n",
        "- input: log 확률\n",
        "- output: class index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ugm5BaicS81v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26dd4e01-9771-4e86-b6b0-513c4f37d49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLLLoss with LogSoftmax: 0.80\n"
          ]
        }
      ],
      "source": [
        "# 모델 출력\n",
        "logits = torch.tensor(\n",
        "\t[[0.1, 0.2, 0.7], # max idx 2\n",
        "     \t [0.8, 0.1, 0.1], # max idx 0\n",
        "     \t [0.3, 0.5, 0.2]], # max idx 1\n",
        "    \trequires_grad=True).float()\n",
        "\n",
        "# 타겟 클래스 인덱스\n",
        "targets = torch.tensor([2, 0, 1]).long()\n",
        "\n",
        "# 로그 소프트맥스 적용\n",
        "log_softmax = F.log_softmax(logits, dim=1)\n",
        "\n",
        "# NLLLoss 선언\n",
        "criterion = nn.NLLLoss()\n",
        "loss = criterion(log_softmax, targets)\n",
        "print(f'NLLLoss with LogSoftmax: {loss.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple-label Classification"
      ],
      "metadata": {
        "id": "BEY06cQDoSGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiLabelSoftMarginLoss\n",
        "### Multi-label Classification을 위한 loss function.\n",
        "- 하나의 sample이 여러 개의 class에 동시에 속할 수 있는 classification.\n",
        "- 각 클래스 레이블을 각각 독립적인 binary classification으로 취급.\n",
        "- input: model의 logit vector output(클래스 각각의 예측값)\n",
        "- 내부적으로 logistic function을 적용하여 이를 [0, 1] 범위의 확률로 변환.\n",
        "- 이 변환된 확률과 실제 레이블의 값을 사용해 binary cross entropy loss 계산해 합침."
      ],
      "metadata": {
        "id": "NmZx-0XwA40_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formula\n",
        "$$L(y, \\hat y)\n",
        " = -\\cfrac1C \\displaystyle \\sum^C_{i=1} [y_i \\cdot \\log(\\sigma(\\hat y_i)) + (1-y_i)\\cdot \\log(1-\\sigma(\\hat y_i))]$$\n",
        "\n",
        "- 𝜎: logistic function (~sigma)\n",
        "- 𝑦̂_𝑖: predicted logit score.\n",
        "- 𝑦_𝑖: label or target."
      ],
      "metadata": {
        "id": "H5MHuFScUKcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class weight for imbalanced class\n",
        "각 클래스에 가중치를 부여할 수 있음\n",
        "- 특정 클래스가 다른 클래스보다 중요\n",
        "- 데이터셋에서 클래스 불균형을 해소할 필요가 있을 때 유용"
      ],
      "metadata": {
        "id": "opT-r9uFUq4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([[0.5, -1.0, 3.0], [1.5, -2.0, 0.0]], requires_grad=True)\n",
        "labels = torch.tensor([[1, 0, 1], [0, 1, 0]], dtype=torch.float)\n",
        "\n",
        "weights = torch.tensor([0.5, 2.0, 1.5], dtype=torch.float) # class 별 중요도\n",
        "\n",
        "loss_func = nn.MultiLabelSoftMarginLoss(weight=weights)\n",
        "\n",
        "loss = loss_func(logits, labels)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtXT8wgKB9WZ",
        "outputId": "e5db3b52-c8c6-4708-ef56-01006670bf76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1801210641860962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid+BCELoss와의 차이점\n",
        "| | MultiLabelSoftMarginLoss|BCELoss|\n",
        "|---|---|---|\n",
        "|sigmoid|내부|외부에서 필요|\n",
        "|classification|multi-label|binary|\n",
        "|input|logit|probability|\n"
      ],
      "metadata": {
        "id": "pOrHX19oVakz"
      }
    }
  ]
}