{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+XggKEQCxxltzYZeYHA2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/PyTorch/blob/main/26-loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNkJ9xK0SZq4"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYlkZr1SfMF"
      },
      "source": [
        "## BCELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35ut6qv2Se40"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 예측값과 실제값\n",
        "outputs = torch.tensor([0.7, 0.2, 0.9], requires_grad=True).float()\n",
        "targets = torch.tensor([1.0, 0.0, 1.0]).float()\n",
        "\n",
        "# BCELoss 선언\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 시그모이드 활성화 적용\n",
        "outputs = torch.sigmoid(outputs)\n",
        "\n",
        "# 손실 계산\n",
        "loss = criterion(outputs, targets)\n",
        "print('BCELoss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0321B4TSpAS"
      },
      "source": [
        "## BCEWithLogitsLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ra2a870SZdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 원시 출력값 (시그모이드 활성화 적용 전 raw score)\n",
        "raw_outputs = torch.tensor([0.5, -1.0, 2.0], requires_grad=True)\n",
        "targets = torch.tensor([1.0, 0.0, 1.0])\n",
        "\n",
        "# BCEWithLogitsLoss 선언\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 손실 계산 (시그모이드 내부 적용)\n",
        "loss = criterion(raw_outputs, targets)\n",
        "print(f'BCEWithLogitsLoss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaI-waGVSsiF"
      },
      "source": [
        "## CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsY8asbLSoOW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 원시 출력값 (softmax 활성화 적용 전)\n",
        "raw_outputs = torch.tensor(\n",
        "\t[[1.0, 2.0, 3.0],\n",
        "         [1.0, 2.0, 0.0],\n",
        "         [0.0, 2.0, 1.0]],\n",
        "        requires_grad=True).float()\n",
        "targets = torch.tensor([2, 0, 1]).long()  # 각 샘플의 클래스 인덱스\n",
        "print(targets.dtype)\n",
        "# CrossEntropyLoss 선언\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 손실 계산 (softmax 내부 적용)\n",
        "loss = criterion(raw_outputs, targets)\n",
        "print(f'CrossEntropyLoss: {loss.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drv_YTj-S35R"
      },
      "source": [
        "## NLLLoss\n",
        "Negative Log Likelihood Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugm5BaicS81v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "# 모델 출력\n",
        "logits = torch.tensor(\n",
        "\t[[0.1, 0.2, 0.7], # max idx 2\n",
        "     \t [0.8, 0.1, 0.1], # max idx 0\n",
        "     \t [0.3, 0.5, 0.2]], # max idx 1\n",
        "    \trequires_grad=True).float()\n",
        "\n",
        "# 타겟 클래스 인덱스\n",
        "targets = torch.tensor([2, 0, 1]).long()\n",
        "\n",
        "# 로그 소프트맥스 적용\n",
        "log_softmax = F.log_softmax(logits, dim=1)\n",
        "\n",
        "# NLLLoss 선언\n",
        "criterion = nn.NLLLoss()\n",
        "loss = criterion(log_softmax, targets)\n",
        "print(f'NLLLoss with LogSoftmax: {loss.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvSDre8NTBPF"
      },
      "source": [
        "앞서 소개한 loss function과 activation function은 PyTorch를 사용하는 딥러닝 모델을 구축하고 효과적으로 훈련하는 데 필수적임.\n",
        "\n",
        "각 함수의 선택과 사용은 model의 특성과 task의 유형에 따라 달라질 수 있음.\n",
        "\n",
        "Binary Classification\n",
        "- BCELoss or BCEWithLogitsLoss\n",
        "\n",
        "Multiple-class Classification:\n",
        "- CrossEntropyLoss 사용\n",
        "- LogSoftmax와 NLLLoss의 조합\n",
        "\n",
        "Activation Function:\n",
        "- Sigmoid는 binary classification(이진 분류)에서\n",
        "- Softmax는 mutiple-class classification(다중 클래스)의 확률을 직접 계산하고,\n",
        "- LogSoftmax는 확률의 로그 값을 계산하여 보다 수치적으로 stable(안정적)인 결과를 제공."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyv0LvV3WYA8"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcTQP7Y1WY8P"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()"
      ]
    }
  ]
}